import gzip
import io
import logging
import random
import xml.etree.ElementTree as ET
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup

from app.schemas.map import MapRequest, LinkResult

logger = logging.getLogger(__name__)

# Rotating headers for HTTP requests
_HEADERS_LIST = [
    {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
    },
    {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
    },
]


async def _fetch_url(url: str, timeout: int = 15) -> tuple[str, int]:
    """Fetch a URL using curl_cffi (TLS impersonation) with httpx fallback."""
    # Try curl_cffi first
    try:
        from curl_cffi.requests import AsyncSession
        impersonate = random.choice(["chrome124", "chrome123", "chrome120"])
        async with AsyncSession(impersonate=impersonate) as session:
            resp = await session.get(
                url,
                timeout=timeout,
                allow_redirects=True,
                headers=random.choice(_HEADERS_LIST),
            )
            logger.debug(f"curl_cffi {url} -> {resp.status_code} ({len(resp.text)} chars)")
            return resp.text, resp.status_code
    except Exception as e:
        logger.debug(f"curl_cffi failed for {url}: {e}")

    # Fallback to httpx
    try:
        async with httpx.AsyncClient(
            timeout=timeout,
            follow_redirects=True,
            headers=random.choice(_HEADERS_LIST),
            http2=True,
        ) as client:
            resp = await client.get(url)
            logger.debug(f"httpx {url} -> {resp.status_code} ({len(resp.text)} chars)")
            return resp.text, resp.status_code
    except Exception as e:
        logger.debug(f"httpx failed for {url}: {e}")

    return "", 0


async def _fetch_bytes(url: str, timeout: int = 15) -> tuple[bytes, int]:
    """Fetch a URL and return raw bytes (for gzip handling)."""
    try:
        from curl_cffi.requests import AsyncSession
        impersonate = random.choice(["chrome124", "chrome123", "chrome120"])
        async with AsyncSession(impersonate=impersonate) as session:
            resp = await session.get(
                url,
                timeout=timeout,
                allow_redirects=True,
                headers=random.choice(_HEADERS_LIST),
            )
            return resp.content, resp.status_code
    except Exception:
        pass

    try:
        async with httpx.AsyncClient(
            timeout=timeout,
            follow_redirects=True,
            headers=random.choice(_HEADERS_LIST),
            http2=True,
        ) as client:
            resp = await client.get(url)
            return resp.content, resp.status_code
    except Exception:
        pass

    return b"", 0


async def _fetch_with_browser(url: str) -> str:
    """Fetch page content using browser for sites that block HTTP requests."""
    try:
        from app.services.browser import browser_pool
        async with browser_pool.get_page(target_url=url) as page:
            referrer = "https://www.google.com/"
            await page.goto(url, wait_until="domcontentloaded", timeout=45000, referer=referrer)
            # Wait for network to settle but don't block forever
            try:
                await page.wait_for_load_state("networkidle", timeout=10000)
            except Exception:
                pass
            await page.wait_for_timeout(random.randint(1000, 2500))
            return await page.content()
    except Exception as e:
        logger.warning(f"Browser fetch failed for {url}: {e}")
        return ""


async def map_website(request: MapRequest) -> list[LinkResult]:
    """
    Discover all URLs on a website using multiple strategies:
    1. Sitemap.xml parsing (with gzip, sitemap index, lastmod/priority)
    2. Quick crawl of homepage + linked pages (with anti-detection)
    3. Browser fallback for blocked sites
    4. Optional search/keyword filtering
    """
    url = request.url
    all_links: dict[str, LinkResult] = {}

    # Strategy 1: Sitemap discovery
    if request.use_sitemap:
        sitemap_links = await _parse_sitemaps(url)
        for link in sitemap_links:
            all_links[link.url] = link

    # Strategy 2: Quick homepage crawl with anti-detection
    homepage_links = await _crawl_homepage(url, request.include_subdomains)
    for link in homepage_links:
        if link.url not in all_links:
            all_links[link.url] = link

    # Strategy 3: If we got very few links, try browser fallback
    if len(all_links) < 5:
        logger.info(f"Few links found for {url}, trying browser fallback")
        browser_links = await _crawl_homepage_browser(url, request.include_subdomains)
        for link in browser_links:
            if link.url not in all_links:
                all_links[link.url] = link

    # Strategy 4: Filter by search term
    if request.search:
        search_lower = request.search.lower()
        filtered = {}
        for url_key, link in all_links.items():
            score = 0
            if search_lower in url_key.lower():
                score += 2
            if link.title and search_lower in link.title.lower():
                score += 3
            if link.description and search_lower in link.description.lower():
                score += 1
            if score > 0:
                filtered[url_key] = link
        all_links = filtered

    # Apply limit
    result = list(all_links.values())[: request.limit]
    return result


async def _parse_sitemaps(base_url: str) -> list[LinkResult]:
    """Parse sitemap.xml and sitemap index files with full spec compliance."""
    links = []
    parsed = urlparse(base_url)
    domain = f"{parsed.scheme}://{parsed.netloc}"

    # Try common sitemap locations
    sitemap_urls = [
        f"{domain}/sitemap.xml",
        f"{domain}/sitemap_index.xml",
        f"{domain}/sitemap/sitemap.xml",
    ]

    # Also check robots.txt for sitemap references
    try:
        text, status = await _fetch_url(f"{domain}/robots.txt", timeout=10)
        if status == 200 and text:
            for line in text.splitlines():
                if line.lower().startswith("sitemap:"):
                    sm_url = line.split(":", 1)[1].strip()
                    if sm_url not in sitemap_urls:
                        sitemap_urls.append(sm_url)
    except Exception:
        pass

    for sitemap_url in sitemap_urls:
        try:
            # Fetch sitemap content (handle gzip)
            xml_text = await _fetch_sitemap_content(sitemap_url)
            if not xml_text:
                continue

            # Use iterparse for streaming XML (memory efficient for large sitemaps)
            root = ET.fromstring(xml_text)
            ns = {
                "sm": "http://www.sitemaps.org/schemas/sitemap/0.9",
                "image": "http://www.google.com/schemas/sitemap-image/1.1",
            }

            # Check if it's a sitemap index
            sitemap_index_entries = root.findall(".//sm:sitemap/sm:loc", ns)
            if sitemap_index_entries:
                # Process up to 200 sub-sitemaps
                for entry in sitemap_index_entries[:200]:
                    if entry.text:
                        sub_xml = await _fetch_sitemap_content(entry.text.strip())
                        if sub_xml:
                            try:
                                sub_root = ET.fromstring(sub_xml)
                                sub_links = _parse_single_sitemap_xml(sub_root, ns)
                                links.extend(sub_links)
                            except Exception:
                                pass
            else:
                # Regular sitemap
                sub_links = _parse_single_sitemap_xml(root, ns)
                links.extend(sub_links)

        except Exception as e:
            logger.debug(f"Failed to parse sitemap {sitemap_url}: {e}")

    return links


async def _fetch_sitemap_content(url: str) -> str | None:
    """Fetch sitemap content, handling gzipped .xml.gz files."""
    if url.endswith(".gz"):
        # Fetch as bytes and decompress
        raw_bytes, status = await _fetch_bytes(url, timeout=20)
        if status != 200 or not raw_bytes:
            return None
        try:
            decompressed = gzip.decompress(raw_bytes)
            return decompressed.decode("utf-8", errors="replace")
        except Exception as e:
            logger.debug(f"Failed to decompress gzipped sitemap {url}: {e}")
            return None
    else:
        text, status = await _fetch_url(url, timeout=15)
        if status != 200 or not text:
            return None
        return text


def _parse_single_sitemap_xml(root: ET.Element, ns: dict) -> list[LinkResult]:
    """Extract URLs from a sitemap XML element with lastmod, priority, and image support."""
    links = []
    for url_el in root.findall(".//sm:url", ns):
        loc = url_el.find("sm:loc", ns)
        if loc is None or not loc.text:
            continue

        url = loc.text.strip()

        # Parse optional fields
        lastmod_el = url_el.find("sm:lastmod", ns)
        lastmod = lastmod_el.text.strip() if lastmod_el is not None and lastmod_el.text else None

        priority_el = url_el.find("sm:priority", ns)
        priority = None
        if priority_el is not None and priority_el.text:
            try:
                priority = float(priority_el.text.strip())
            except ValueError:
                pass

        changefreq_el = url_el.find("sm:changefreq", ns)
        changefreq = changefreq_el.text.strip() if changefreq_el is not None and changefreq_el.text else None

        # Parse image sitemap entries
        image_urls = []
        for img_el in url_el.findall("image:image/image:loc", ns):
            if img_el.text:
                image_urls.append(img_el.text.strip())

        # Build description from metadata
        desc_parts = []
        if lastmod:
            desc_parts.append(f"Updated: {lastmod}")
        if changefreq:
            desc_parts.append(f"Freq: {changefreq}")
        if priority is not None:
            desc_parts.append(f"Priority: {priority}")
        if image_urls:
            desc_parts.append(f"{len(image_urls)} image(s)")

        description = " | ".join(desc_parts) if desc_parts else None

        links.append(LinkResult(
            url=url,
            title=None,
            description=description,
            lastmod=lastmod,
            priority=priority,
        ))

    return links


async def _crawl_homepage(base_url: str, include_subdomains: bool) -> list[LinkResult]:
    """Quick crawl of homepage using curl_cffi for anti-detection."""
    links = []
    parsed_base = urlparse(base_url)
    base_domain = parsed_base.netloc
    if base_domain.startswith("www."):
        base_domain_nowww = base_domain[4:]
    else:
        base_domain_nowww = base_domain

    try:
        text, status = await _fetch_url(base_url, timeout=20)
        if not text:
            return links

        links = _extract_links_from_html(text, base_url, base_domain, include_subdomains)
        logger.info(f"Homepage crawl for {base_url}: status={status}, links={len(links)}")

    except Exception as e:
        logger.warning(f"Homepage crawl failed for {base_url}: {e}")

    return links


async def _crawl_homepage_browser(base_url: str, include_subdomains: bool) -> list[LinkResult]:
    """Crawl homepage using browser for sites that block HTTP requests."""
    parsed_base = urlparse(base_url)
    base_domain = parsed_base.netloc

    try:
        html = await _fetch_with_browser(base_url)
        if not html:
            return []

        return _extract_links_from_html(html, base_url, base_domain, include_subdomains)

    except Exception as e:
        logger.warning(f"Browser homepage crawl failed for {base_url}: {e}")
        return []


def _extract_links_from_html(
    html: str, base_url: str, base_domain: str, include_subdomains: bool
) -> list[LinkResult]:
    """Extract all links from HTML content."""
    links = []
    soup = BeautifulSoup(html, "lxml")

    # Get page title and description for context
    page_title = soup.title.get_text(strip=True) if soup.title else None

    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"].strip()
        if href.startswith(("#", "mailto:", "tel:", "javascript:")):
            continue

        absolute_url = urljoin(base_url, href)
        parsed = urlparse(absolute_url)

        # Only http/https
        if parsed.scheme not in ("http", "https"):
            continue

        # Filter by domain
        if not include_subdomains and parsed.netloc != base_domain:
            continue
        if include_subdomains:
            # Allow subdomains of the same root domain
            base_parts = base_domain.split(".")
            parsed_parts = parsed.netloc.split(".")
            if len(base_parts) >= 2 and len(parsed_parts) >= 2:
                base_root = ".".join(base_parts[-2:])
                parsed_root = ".".join(parsed_parts[-2:])
                if base_root != parsed_root:
                    continue

        # Clean URL (remove fragments)
        clean_url = parsed._replace(fragment="").geturl()

        title = a_tag.get_text(strip=True) or None

        # Get description from nearby text or parent
        description = None
        parent = a_tag.parent
        if parent:
            sibling_text = parent.get_text(strip=True)
            if sibling_text and sibling_text != title and len(sibling_text) < 200:
                description = sibling_text

        links.append(LinkResult(url=clean_url, title=title, description=description))

    return links
